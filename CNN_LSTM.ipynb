{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import utils as np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Embedding, Merge, LSTM, GRU\n",
    "from keras.layers import Input, Dense, Dropout, Reshape\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Flatten\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('data/reduced_data.pkl','rb')\n",
    "data=pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data,columns=['data','labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df['data'].tolist()\n",
    "labels=df['labels'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize the data (strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences = tokenizer.texts_to_sequences(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn target variables to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1={}\n",
    "inc=0\n",
    "for i in range(0,len(labels)):\n",
    "    if(labels[i] in dict1.keys()):\n",
    "        labels[i]=dict1[labels[i]]\n",
    "    else:\n",
    "        inc+=1\n",
    "        dict1[labels[i]]=inc\n",
    "        labels[i]=dict1[labels[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array([np.array(xi) for xi in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen = 81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (222768, 81)\n",
      "Shape of label tensor: (222768, 14)\n"
     ]
    }
   ],
   "source": [
    "labels = np_utils.to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide in train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT=0.2\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the word vectors (Glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR='glove/'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt'), encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the data sentences to word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=300\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "null_vectors=0\n",
    "total=0\n",
    "for word, i in word_index.items():\n",
    "    total+=1\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        null_vectors+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH=81\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mugdha_ans\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=200, kernel_size=3)`\n",
      "  \n",
      "c:\\users\\mugdha_ans\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=200, kernel_size=4)`\n",
      "  \n",
      "c:\\users\\mugdha_ans\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=200, kernel_size=5)`\n",
      "  \n",
      "c:\\users\\mugdha_ans\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "convs = []\n",
    "filter_sizes = [3,4,5]\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "for fsz in filter_sizes:\n",
    "    l_conv = Conv1D(nb_filter=200,filter_length=fsz,activation='relu')(embedded_sequences)\n",
    "    l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "    convs.append(l_pool)\n",
    "l_merge = Merge(mode='concat')(convs)\n",
    "l_merge_r = Reshape((600,1))(l_merge)\n",
    "lstm_out = GRU(128) (l_merge_r)\n",
    "preds = Dense(14, activation='softmax')(lstm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 81)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 81, 300)      5978700     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 79, 200)      180200      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 78, 200)      240200      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 77, 200)      300200      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 200)          0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 200)          0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 200)          0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "merge_6 (Merge)                 (None, 600)          0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "                                                                 global_max_pooling1d_18[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 600, 1)       0           merge_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, 128)          49920       reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 14)           1806        gru_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 6,751,026\n",
      "Trainable params: 772,326\n",
      "Non-trainable params: 5,978,700\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 178215 samples, validate on 44553 samples\n",
      "Epoch 1/1\n",
      "178215/178215 [==============================] - 8456s 47ms/step - loss: 0.8782 - acc: 0.6604 - val_loss: 0.8150 - val_acc: 0.6799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cee3480080>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(sequence_input, preds)\n",
    "model.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
