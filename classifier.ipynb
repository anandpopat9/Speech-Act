{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file='../data/merged.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from __future__ import division\n",
    "import inspect\n",
    "from swa import Transcript\n",
    "from os import listdir\n",
    "from time import time\n",
    "from re import findall, sub\n",
    "from sklearn import tree\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "import pydotplus\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import enchant\n",
    "import string\n",
    "import itertools\n",
    "from stemming.porter2 import stem\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BagOfWords:\n",
    "    def __init__(self):\n",
    "        self.space = Counter()\n",
    "\n",
    "    def populateSpace(self, data):\n",
    "        # generate space for bag of words on all the data\n",
    "        # space => {'unique_word': (unique_index_for_word, <number_of_occurences>)..}\n",
    "        # data\n",
    "        for utter in data:\n",
    "            # utter.text\n",
    "            for token in utter.tokens:\n",
    "               if token in ['{', '}', '[', ']', '/']:  # ignore literals\n",
    "                   continue\n",
    "               if self.space[token] == 0:\n",
    "                    self.space[token] = [len(self.space), 1]\n",
    "               else:\n",
    "                    self.space[token][1] += 1\n",
    "        #print self.space\n",
    "\n",
    "\n",
    "    def featurize(self, utterances):\n",
    "        speec_acts=[]\n",
    "        utter_text=[]\n",
    "        feature_vectors=[]\n",
    "        feature_vectors_bow = open(\"pkl/feature_vectors_bow.pkl\",\"wb\")\n",
    "        speec_acts_bow = open(\"pkl/speec_acts_bow.pkl\",\"wb\")\n",
    "        utter_text_bow = open(\"pkl/utter_text_bow.pkl\",\"wb\")\n",
    "        counter=0\n",
    "        print \"-----------bow space---------\", len(self.space)\n",
    "        # form feature vector for sentences\n",
    "        for utter in utterances:\n",
    "            #print utter\n",
    "            #utterTokens = word_tokenize(utter.text)\n",
    "            feature_vector_utter = [0] * len(self.space)\n",
    "            \n",
    "            for utterToken in utter.tokens:\n",
    "                if utterToken in ['{', '}', '[', ']', '/']:  # ignore literals\n",
    "                   continue\n",
    "                if self.space[utterToken] != 0:\n",
    "                    feature_vector_utter[self.space[utterToken][0]] = 1     # get the unique index of the word in space\n",
    "            counter+=1\n",
    "            if(counter % 1000==0):\n",
    "                print counter\n",
    "            \n",
    "            speec_acts.append(utter.act_tag)\n",
    "            utter_text.append(utter.text)\n",
    "            feature_vectors.append(feature_vector_utter)\n",
    "\n",
    "        pickle.dump(speec_acts, speec_acts_bow)\n",
    "        pickle.dump(utter.text, utter_text_bow)\n",
    "        pickle.dump(feature_vectors, feature_vectors_bow)\n",
    "            \n",
    "        feature_vectors_bow.close()\n",
    "        speec_acts_bow.close()\n",
    "        utter_text_bow.close()\n",
    "            #speec_acts.append(utter.act_tag)\n",
    "            #utter_text.append(utter.text)\n",
    "            #feature_vectors.append(feature_vector_utter)\n",
    "            #print feature_vectors\n",
    "        #return feature_vectors, speec_acts, utter_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Feature:\n",
    "    def __init__(self, utterance, previousUtterance_act_tag):\n",
    "        self.utterance = utterance\n",
    "        self.previousUtterance_act_tag = previousUtterance_act_tag\n",
    "        self.featureHeaders = [\n",
    "            'question_mark',        # check for presence of question mark\n",
    "            'wh_question',          # check for presence of wh- question words\n",
    "            'i_dont_know',          # check for presence of phrase 'i don't know'\n",
    "            'no_words',             # check for presence of \"No\" words\n",
    "            'yes_words',            # check for presence of \"Yes\" words\n",
    "            'do_words',             # check for presence of tense of \"do\" - did, does\n",
    "            'non_verbal',           # check for presence of non-verbal words, < action >\n",
    "            'UH_count',             # check for presence of Interjection (UH) Parts of speech in the sentence\n",
    "            #'CC_count',             # check for presence of co-ordinating conjunction (CC)\n",
    "            'thanking_words',       # check for presence of words expressing \"Thanks\"\n",
    "            'apology_words',        # check for presence of words\n",
    "            #'sub_utterance_index',  # add sub-utterance index\n",
    "            #'utterance_index',      # add utterance index\n",
    "            #'utterance_count'       # add conversation length\n",
    "            'qrr_sequence'          # check for presence of speech tag \"q<x>\" in previous utterance and current occurence\n",
    "        ]\n",
    "\n",
    "        self.featureKeys = {\n",
    "            \"question_mark\" :       '?',\n",
    "            \"wh_question\"   :       ['who', 'which', 'where', 'what', 'how'],\n",
    "            \"i_dont_know\"   :       [\"i don't know\"],\n",
    "            \"no_words\"      :       [\"no\", \"nah\"],\n",
    "            \"yes_words\"     :       [\"yes\", \"yeah\"],\n",
    "            \"do_words\"      :       [\"do\", \"did\", \"does\"],\n",
    "            \"non_verbal\"    :       '^<.*?>',\n",
    "            \"UH_count\"      :       '/UH',\n",
    "            #\"CC_count\"      :       '/CC',\n",
    "            \"thanking_words\":       ['thank', 'thanks', 'thank you'],\n",
    "            \"apology_words\" :       ['sorry', 'apology'],\n",
    "            \"qrr_sequence\"  :       ['qw', 'qh', 'qo', 'qr']\n",
    "        }\n",
    "\n",
    "    def qrr_sequence(self):\n",
    "        if len(self.previousUtterance_act_tag) != 0 and (self.previousUtterance_act_tag in self.featureKeys[inspect.currentframe().f_code.co_name]):\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def question_mark(self):\n",
    "        if self.featureKeys[inspect.currentframe().f_code.co_name] in self.utterance.text:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def wh_question(self):\n",
    "        tag_word_count = 0\n",
    "        for tag_word in self.featureKeys[inspect.currentframe().f_code.co_name]:\n",
    "            if findall('\\\\b'+tag_word+'\\\\b', self.utterance.text):\n",
    "                tag_word_count += 1\n",
    "        return tag_word_count\n",
    "\n",
    "    def i_dont_know(self):\n",
    "        tag_word_count = 0\n",
    "        for tag_word in self.featureKeys[inspect.currentframe().f_code.co_name]:\n",
    "            if findall('\\\\b'+tag_word+'\\\\b', self.utterance.text):\n",
    "                tag_word_count += 1\n",
    "        return tag_word_count\n",
    "\n",
    "    def no_words(self):\n",
    "        tag_word_count = 0\n",
    "        for tag_word in self.featureKeys[inspect.currentframe().f_code.co_name]:\n",
    "            if findall('\\\\b'+tag_word+'\\\\b', self.utterance.text):\n",
    "                tag_word_count += 1\n",
    "        return tag_word_count\n",
    "\n",
    "    def yes_words(self):\n",
    "        tag_word_count = 0\n",
    "        for tag_word in self.featureKeys[inspect.currentframe().f_code.co_name]:\n",
    "            if findall('\\\\b'+tag_word+'\\\\b', self.utterance.text):\n",
    "                tag_word_count += 1\n",
    "        return tag_word_count\n",
    "\n",
    "    def do_words(self):\n",
    "        tag_word_count = 0\n",
    "        for tag_word in self.featureKeys[inspect.currentframe().f_code.co_name]:\n",
    "            if findall('\\\\b'+tag_word+'\\\\b', self.utterance.text):\n",
    "                tag_word_count += 1\n",
    "        return tag_word_count\n",
    "\n",
    "    def non_verbal(self):\n",
    "        # search for string <abcde>,\n",
    "        #  ^ -> start of sentence, non-greedy pattern <.*?>\n",
    "        return len(findall(self.featureKeys[inspect.currentframe().f_code.co_name], self.utterance.text))\n",
    "\n",
    "    def UH_count(self):\n",
    "        # maybe, check for length of text; if length less than 2 then return true? - Skepticism :-/\n",
    "        if len(self.utterance.pos.split()) < 3 and \\\n",
    "                self.featureKeys[inspect.currentframe().f_code.co_name] in self.utterance.pos:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def CC_count(self):\n",
    "        if len(self.utterance.pos.split()) < 3 and \\\n",
    "                self.featureKeys[inspect.currentframe().f_code.co_name] in self.utterance.pos:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def thanking_words(self):\n",
    "        tag_word_count = 0\n",
    "        for tag_word in self.featureKeys[inspect.currentframe().f_code.co_name]:\n",
    "            if findall('\\\\b'+tag_word+'\\\\b', self.utterance.text):\n",
    "                tag_word_count += 1\n",
    "        return tag_word_count\n",
    "\n",
    "    def apology_words(self):\n",
    "        tag_word_count = 0\n",
    "        for tag_word in self.featureKeys[inspect.currentframe().f_code.co_name]:\n",
    "            if findall('\\\\b'+tag_word+'\\\\b', self.utterance.text):\n",
    "                tag_word_count += 1\n",
    "        return tag_word_count\n",
    "\n",
    "    def sub_utterance_index(self):\n",
    "            return self.utterance.subutterance_index\n",
    "\n",
    "    def utterance_index(self):\n",
    "            return self.utterance.utterance_index\n",
    "\n",
    "    def utterance_count(self):\n",
    "            return self.utterance.utterance_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, dataset, datasetPath):\n",
    "        self.dataName = dataset\n",
    "        self.datasetPath = datasetPath\n",
    "        self.data = []\n",
    "        self.totalDataCount = 0\n",
    "        self.trainData = []\n",
    "        self.testData = []\n",
    "        self.trainPercentage = 90\n",
    "        self.testPercentage = 10\n",
    "        self.speech_acts_class = [\n",
    "            'sd',\n",
    "            'b',\n",
    "            'sv',\n",
    "            #'aa',\n",
    "            'qy',\n",
    "            'x',\n",
    "            'ny',\n",
    "            'qw',\n",
    "            'nn',\n",
    "            'h',\n",
    "            'qy^d',\n",
    "            #'qw^d',\n",
    "            'fa',\n",
    "            'ft'\n",
    "        ]\n",
    "        self.speech_acts_class = self.speechActDictify()\n",
    "\n",
    "    def speechActDictify(self):\n",
    "        speech_acts_class = Counter()\n",
    "        for speech_act in self.speech_acts_class:\n",
    "            speech_acts_class[speech_act] = 1\n",
    "\n",
    "        return speech_acts_class\n",
    "\n",
    "    def getData(self):\n",
    "        # list directories for dataset files\n",
    "        #for dir in listdir(self.datasetPath):\n",
    "         #   if dir.startswith('.'):\n",
    "          #      continue\n",
    "\n",
    "            #print dir\n",
    "           # for file in listdir(self.datasetPath + dir):\n",
    "            #    if file.startswith('.'):\n",
    "             #       continue\n",
    "               # dataFile = self.datasetPath + dir + '/' + file\n",
    "                trans = Transcript(self.datasetPath)\n",
    "                #getdata=open(\"getdata.pkl\",\"wb\")\n",
    "                #pickle.dump(trans.utterances, getdata)\n",
    "                self.data.extend(trans.utterances)\n",
    "                self.totalDataCount += len(trans.utterances)\n",
    "\n",
    "    def getTrainAndTestData(self):\n",
    "        self.trainData = self.data[:int(self.trainPercentage/100 * self.totalDataCount)]\n",
    "        self.testData = self.data[-int(self.testPercentage/100 * self.totalDataCount):]\n",
    "\n",
    "    def featurize(self, utterances):\n",
    "        #feature_vectors = []\n",
    "        #speec_acts = []\n",
    "        #utter_text = []\n",
    "        feature_vectors1 = open(\"feature_vectors.pkl\",\"wb\")\n",
    "        speec_acts1 = open(\"speec_acts.pkl\",\"wb\")\n",
    "        utter_text1 = open(\"utter_text.pkl\",\"wb\")\n",
    "        \n",
    "        previousUtter = ''\n",
    "        # form feature vector for sentences\n",
    "        for utter in utterances:\n",
    "            feature = Feature(utter, previousUtter)\n",
    "            #feature_vector = {}\n",
    "            feature_vector_utter = []\n",
    "            for headers in feature.featureHeaders:\n",
    "                #feature_vector[headers] = getattr(feature, headers)()\n",
    "                feature_vector_utter.append(getattr(feature, headers)())\n",
    "            pickle.dump(utter.act_tag, speec_acts1)\n",
    "            pickle.dump(utter.text, utter_text1)\n",
    "            pickle.dump(feature_vector_utter, feature_vectors1)\n",
    "            \n",
    "            #speec_acts.append(utter.act_tag)\n",
    "            #utter_text.append(utter.text)\n",
    "            #feature_vectors.append(feature_vector_utter)\n",
    "            previousUtter = utter.act_tag\n",
    "            #feature_vectors.append([feature_vector[key] for key in feature_vector])\n",
    "            #print utter.text, feature_vector\n",
    "        feature_vectors1.close()\n",
    "        speec_acts1.close()\n",
    "        utter_text1.close()\n",
    "        #return feature_vectors, speec_acts, utter_text\n",
    "\n",
    "    def normalizeSpeechAct(self, speechActs):\n",
    "        # normalize speech_acts\n",
    "        for speechActIndex in range(len(speechActs)):\n",
    "            trimSpeechAct = sub('\\^2|\\^g|\\^m|\\^r|\\^e|\\^q|\\^d', '',speechActs[speechActIndex])\n",
    "            if self.speech_acts_class[speechActs[speechActIndex]] != 1 or \\\n",
    "                trimSpeechAct in ['sd', 'sv'] or \\\n",
    "                    self.speech_acts_class[trimSpeechAct] != 1:\n",
    "                 #speechActs[speechActIndex] = 'other'\n",
    "                 speechActs[speechActIndex] = 's'\n",
    "\n",
    "    def normalizeSpeechActTest(self, speechActs):\n",
    "        # normalize speech_acts\n",
    "        for speechActIndex in range(len(speechActs)):\n",
    "            trimSpeechAct = sub('\\^2|\\^g|\\^m|\\^r|\\^e|\\^q|\\^d', '',speechActs[speechActIndex])\n",
    "            if trimSpeechAct in ['sd', 'sv']:\n",
    "                speechActs[speechActIndex] = 's'\n",
    "            elif self.speech_acts_class[speechActs[speechActIndex]] != 1 or \\\n",
    "                    self.speech_acts_class[trimSpeechAct] != 1:\n",
    "                 speechActs[speechActIndex] = 'rest'\n",
    "\n",
    "    def normalizePrediction(self, predicted_speech_act, labelledSpeechAct):\n",
    "        for i in range(len(labelledSpeechAct)):\n",
    "            if labelledSpeechAct[i] == 'rest' and predicted_speech_act[i] == 's':\n",
    "                predicted_speech_act[i] = 'rest'\n",
    "\n",
    "    def combineFeatureVectors(self, feature_vectors_bow, feature_vectors_cust):\n",
    "        feature_vectors = []\n",
    "        for i in range(len(feature_vectors_bow)):\n",
    "            #feature_vectors.append(feature_vectors_bow[i] + feature_vectors_cust[i])\n",
    "            feature_vectors.append(list(itertools.chain(feature_vectors_bow[i],feature_vectors_cust[i])))\n",
    "        return feature_vectors\n",
    "\n",
    "    def findmajorityclass(self,speech_act):\n",
    "        class_dist=Counter(speech_act)\n",
    "        majority_class=class_dist.most_common(1)\n",
    "    \tprint \"Majority class\", majority_class\n",
    "    \tcount=majority_class[0]\n",
    "    \tprint \"Majority percentage: \",100*count[1]/len(speech_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = Classifier('swa', data_file)\n",
    "bagofwords = BagOfWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bagofwords = BagOfWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.getData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "////////////////////storing the train and the test data in pickle file//////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.getTrainAndTestData()\n",
    "value=[]\n",
    "for i in range(0,len(classifier.trainData)):   \n",
    "    temp=[]\n",
    "    #train_data[1]#utterance.text\n",
    "    for t in classifier.trainData[i].tokens:\n",
    "        if t in ['{', '}', '[', ']', '/','+','>','<','-']:\n",
    "            continue\n",
    "        else:\n",
    "            temp.append(t)\n",
    "    value.append(' '.join(temp))\n",
    "train_data = open(\"pkl/train_data.pkl\",\"wb\")\n",
    "pickle.dump(value,train_data)\n",
    "train_data.close()\n",
    "value=[]\n",
    "for i in range(0,len(classifier.testData)):   \n",
    "    temp=[]\n",
    "    #train_data[1]#utterance.text\n",
    "    for t in classifier.testData[i].tokens:\n",
    "        if t in ['{', '}', '[', ']', '/','+','>','<','-']:\n",
    "            continue\n",
    "        else:\n",
    "            temp.append(t)\n",
    "    value.append(' '.join(temp))\n",
    "test_data = open(\"pkl/test_data.pkl\",\"wb\")\n",
    "pickle.dump(value,test_data)\n",
    "test_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "value=[]\n",
    "for i in range(0,len(classifier.trainData)):   \n",
    "    value.append(classifier.trainData[i].act_tag)\n",
    "train_target = open(\"pkl/train_target.pkl\",\"wb\")\n",
    "pickle.dump(value,train_target)\n",
    "train_target.close()\n",
    "value=[]\n",
    "for i in range(0,len(classifier.testData)):   \n",
    "    value.append(classifier.testData[i].act_tag)\n",
    "test_target = open(\"pkl/test_target.pkl\",\"wb\")\n",
    "pickle.dump(value,test_target)\n",
    "test_target.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class garzon(object):\n",
    "    def __init__(self,data):\n",
    "        t=[]\n",
    "        tag=[]\n",
    "        for token in data:\n",
    "            b=token.pos\n",
    "            tag1=token.act_tag\n",
    "            t.append(token.pos)\n",
    "            tag.append(tag1)\n",
    "        print len(tag)\n",
    "    \n",
    "        #without=b.translate(None,string.punctuation)\n",
    "\n",
    "        ngrams=[]\n",
    "        for i in range(0,len(t)):\n",
    "            without=[]\n",
    "            a=t[i].split('/')\n",
    "            question=False\n",
    "            for word in a:\n",
    "                #add question mark or use any other method to check for question mark\n",
    "                #print word.translate(None,string.punctuation).split(' ')\n",
    "                without.append(word.translate(None,string.punctuation).split(' '))\n",
    "        #print anand\n",
    "\n",
    "\n",
    "            flat_list = [item for sublist in without for item in sublist]\n",
    "            my_list = [x for x in flat_list if not x == '']\n",
    "            my_list_1=[my_list[i] for i in range(0,len(my_list),2)]\n",
    "            my_list_3=[stem(my_list_1[i]) for i in range(0,len(my_list_1))]\n",
    "            my_list_2=[my_list[i] for i in range(1,len(my_list),2)]\n",
    "            if(not my_list_3 or not my_list_2):\n",
    "                my_list_3=['NA']\n",
    "                my_list_2=['NA']\n",
    "                \n",
    "            word_pos=zip(my_list_3,my_list_2)\n",
    "            ngrams.append(word_pos)\n",
    "        print len(ngrams)\n",
    "        zipped=zip(ngrams,tag)\n",
    "        #print zipped\n",
    "        w,tags=zip(*zipped)\n",
    "        self.w=w\n",
    "        for i in range(0,len(self.w)):\n",
    "            if(i==34):\n",
    "                print self.w[34]\n",
    "        \n",
    "    def featurize(self):\n",
    "        feat=[]\n",
    "        for i in range(0,len(self.w)):\n",
    "            feature=[0] * 5\n",
    "            #print i\n",
    "            word,pos=zip(*self.w[i])\n",
    "            pos=list(pos)\n",
    "            word=list(word)\n",
    "            if ('PRP' in pos):\n",
    "                feature[0]=1\n",
    "            if((pos[0] in ('MD','VB','VBD','VBG','VBN','VBP','VBZ')) and word[0] != 'JOIN' and word[0] != 'PART'):\n",
    "                feature[1]=1\n",
    "            for i in range(0,len(pos)-1):\n",
    "                if pos[i] == 'TO' and (pos[i+1] == 'VB' or pos[i+1] == 'VBP'):\n",
    "                    feature[2]=1\n",
    "            if pos[0] in ('WP','WRB','WDT'):\n",
    "                feature[3]=1\n",
    "            if pos[len(pos)-1] in ('NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS'):\n",
    "                feature[4]=1\n",
    "            feat.append(feature)\n",
    "    \n",
    "        return feat\n",
    "    #o,p=zip(*w[1])\n",
    "    #print p\n",
    "    \n",
    "    #bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.getTrainAndTestData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bagofwords.populateSpace(classifier.trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------bow space--------- 5514\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "bagofwords.featurize(classifier.trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_vectors_bow = []\n",
    "infile = open('pkl/feature_vectors_bow.pkl', 'rb')\n",
    "while 1:\n",
    "    try:\n",
    "        feature_vectors_bow.append(pickle.load(infile))\n",
    "    except (EOFError):\n",
    "        break\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_vectors_bow[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12910\n"
     ]
    }
   ],
   "source": [
    "feature_vectors_cust, speech_acts, utter_text = classifier.featurize(classifier.trainData)\n",
    "print len(feature_vectors_cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12910\n",
      "12910\n",
      "[('NA', 'NA')]\n",
      "12910\n"
     ]
    }
   ],
   "source": [
    "feature_vectors_garzon=garzon(classifier.trainData).featurize()\n",
    "print len(feature_vectors_garzon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//concatenate feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_vectors = classifier.combineFeatureVectors(feature_vectors_bow, feature_vectors_cust)\n",
    "feature_vectors = classifier.combineFeatureVectors(feature_vectors, feature_vectors_garzon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//defines the 's' class which is undefined----go through this again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.normalizeSpeechAct(speech_acts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//see if this is creates problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority class [('s', 10145)]\n",
      "Majority percentage:  78.5824941905\n"
     ]
    }
   ],
   "source": [
    "classifier.findmajorityclass(speech_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "#clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5530"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = clf.fit(feature_vectors, speech_acts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1434\n",
      "1434\n",
      "[('Uhhuh', 'UH')]\n"
     ]
    }
   ],
   "source": [
    "feature_vectors_bow, labelled_speech_acts, utter_text = bagofwords.featurize(classifier.testData)\n",
    "feature_vectors_cust, speech_acts, utter_text = classifier.featurize(classifier.testData)\n",
    "feature_vectors_garzon=garzon(classifier.testData).featurize()\n",
    "feature_vectors = classifier.combineFeatureVectors(feature_vectors_bow, feature_vectors_cust)\n",
    "feature_vectors = classifier.combineFeatureVectors(feature_vectors,feature_vectors_garzon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.normalizeSpeechActTest(labelled_speech_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_speech_act = clf.predict(feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1434"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.normalizePrediction(predicted_speech_act, labelled_speech_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1433\n",
      "total_correct 1249\n",
      "total wrong 185\n",
      "accuracy 87.0990237099\n",
      "Classification_report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          b       0.70      0.88      0.78       239\n",
      "          h       0.37      0.64      0.47        11\n",
      "         nn       1.00      0.11      0.20         9\n",
      "         ny       0.00      0.00      0.00        19\n",
      "         qw       0.57      0.57      0.57        14\n",
      "         qy       0.76      0.73      0.75        30\n",
      "       qy^d       0.11      0.25      0.15         4\n",
      "       rest       1.00      0.82      0.90       493\n",
      "          s       0.91      0.97      0.94       592\n",
      "          x       1.00      0.96      0.98        23\n",
      "\n",
      "avg / total       0.88      0.87      0.87      1434\n",
      "\n",
      "accuracy_score: 0.87\n",
      "b: Acknowledge (Backchannel), fa: Apology, ft: Thanking, h: Hedge, nn: No answers, ny: Yes answers, qw: Wh-Question, qy: Yes-No-Question, qy^d: Declarative Yes-No-Question, rest, s: statement opinion & statement non-opinion, x: Non-verbal\n"
     ]
    }
   ],
   "source": [
    "correctResult = Counter()\n",
    "wrongResult = Counter()\n",
    "\n",
    "i=0\n",
    "for i in range(len(predicted_speech_act)):\n",
    "    if predicted_speech_act[i] == labelled_speech_acts[i]:\n",
    "        correctResult[predicted_speech_act[i]] += 1\n",
    "    else:\n",
    "        wrongResult[predicted_speech_act[i]] += 1\n",
    "print i\n",
    "\n",
    "total_correct = sum([correctResult[i] for i in correctResult])\n",
    "total_wrong = len(predicted_speech_act) - total_correct\n",
    "\n",
    "print \"total_correct\", total_correct\n",
    "print \"total wrong\", total_wrong\n",
    "print \"accuracy\", (total_correct/len(predicted_speech_act)) * 100\n",
    "\n",
    "print \"Classification_report:\\n\", classification_report(labelled_speech_acts, predicted_speech_act)#, target_names=target_names)\n",
    "print \"accuracy_score:\", round(accuracy_score(labelled_speech_acts, predicted_speech_act), 2)\n",
    "print \"b: Acknowledge (Backchannel), fa: Apology, ft: Thanking, h: Hedge, nn: No answers, ny: Yes answers, qw: Wh-Question, qy: Yes-No-Question, qy^d: Declarative Yes-No-Question, rest, s: statement opinion & statement non-opinion, x: Non-verbal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//accuracy score decreases significantly if sv and sd both are used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//features usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features usage\n",
      "---------------------------------------\n",
      "question-mark:  607\n",
      "wh_question:  996\n",
      "i_dont_know:  154\n",
      "no_words:  229\n",
      "yes_words:  1485\n",
      "do_words:  857\n",
      "non_verbal:  458\n",
      "UH_count:  1762\n",
      "thanking_words:  4\n",
      "apology_words:  8\n",
      "qrr_sequence:  252\n"
     ]
    }
   ],
   "source": [
    "print \"features usage\"\n",
    "print \"---------------------------------------\"\n",
    "feature_vectors_cust, speech_acts, utter_text = classifier.featurize(classifier.data)\n",
    "total={}\n",
    "for headers in range(0,len(feature_vectors_cust[1])):\n",
    "    total[headers]=0\n",
    "    for i in range(0,len(feature_vectors_cust)):\n",
    "        if(feature_vectors_cust[i][headers]==1):\n",
    "            total[headers]+=1\n",
    "print \"question-mark: \", total[0]\n",
    "print \"wh_question: \",total[1]\n",
    "print \"i_dont_know: \",total[2]\n",
    "print \"no_words: \",total[3]\n",
    "print \"yes_words: \",total[4]\n",
    "print \"do_words: \",total[5]\n",
    "print \"non_verbal: \",total[6]\n",
    "print \"UH_count: \",total[7]\n",
    "print \"thanking_words: \",total[8]\n",
    "print \"apology_words: \",total[9]\n",
    "print \"qrr_sequence: \",total[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
